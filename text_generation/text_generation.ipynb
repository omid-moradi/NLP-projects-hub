{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ALS6ACfP-TPI",
        "outputId": "5f86c21d-905b-4afd-ea52-74742db3bca5"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "yG_n40gFzf9s"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "pujMhnz2-RC6"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import os\n",
        "import time"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UHjdCjDuSvX_"
      },
      "source": [
        "### Read the data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "aavnuByVymwK"
      },
      "outputs": [],
      "source": [
        "text_path = \"/content/drive/MyDrive/Colab Notebooks/NLP/2/10-Text Generator(Attar's Poem)/Sample/naserkhosro.txt\"\n",
        "text = open(text_path, 'rb').read().decode(encoding='utf-8')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gfnnmEFe-RC_",
        "outputId": "21c2bd2f-5cb6-403a-d795-2e42ba770bf3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Length of text: 633429 characters\n"
          ]
        }
      ],
      "source": [
        "# length of text is the number of characters in it\n",
        "print ('Length of text: {} characters'.format(len(text)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Duhg9NrUymwO",
        "outputId": "8f06f7fa-3ddd-430c-a796-0e52c5ff75ba"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ای قبهٔ گردندهٔ بی روزن خضرا\n",
            "با قامت فرتوتی و با قوت برنا\n",
            "فرزند توایم ای فلک، ای مادر بدمهر\n",
            "ای مادر ما چونکه همی کین کشی از ما؟\n",
            "فرزند تو این تیره تن خامش خاکی است\n",
            "پاکیزه خرد نیست نه این جوهر گویا\n",
            "تن خانهٔ این گوهر والای شریف است\n",
            "تو مادر این خانهٔ این\n"
          ]
        }
      ],
      "source": [
        "# Take a look at the first 250 characters in text\n",
        "print(text[:250])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IlCgQBRVymwR",
        "outputId": "7d45e787-d182-489b-a1cb-35b996c7214a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "50 unique characters\n"
          ]
        }
      ],
      "source": [
        "# The unique characters in the file\n",
        "vocab = sorted(set(text))\n",
        "print('{} unique characters'.format(len(vocab)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rNnrKn_lL-IJ"
      },
      "source": [
        "## Process the text"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LFjSVAlWzf-N"
      },
      "source": [
        "### Vectorize the text\n",
        "\n",
        "Before training, we need to map strings to a numerical representation. Create two lookup tables: one mapping characters to numbers, and another for numbers to characters."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "IalZLbvOzf-F"
      },
      "outputs": [],
      "source": [
        "# Creating a mapping from unique characters to indices\n",
        "char2idx = {u:i for i, u in enumerate(vocab)}\n",
        "idx2char = np.array(vocab)\n",
        "\n",
        "text_as_int = np.array([char2idx[c] for c in text])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tZfqhkYCymwX"
      },
      "source": [
        "Now we have an integer representation for each character. Notice that we mapped the character as indexes from 0 to `len(unique)`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FYyNlCNXymwY",
        "outputId": "00e15e3b-d383-4f2e-e826-322e3b22c2b4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\n",
            "  '\\n':   0,\n",
            "  ' ' :   1,\n",
            "  '!' :   2,\n",
            "  '(' :   3,\n",
            "  ')' :   4,\n",
            "  '.' :   5,\n",
            "  ':' :   6,\n",
            "  '«' :   7,\n",
            "  '»' :   8,\n",
            "  '،' :   9,\n",
            "  '؛' :  10,\n",
            "  '؟' :  11,\n",
            "  'ء' :  12,\n",
            "  'آ' :  13,\n",
            "  'ؤ' :  14,\n",
            "  'ئ' :  15,\n",
            "  'ا' :  16,\n",
            "  'ب' :  17,\n",
            "  'ة' :  18,\n",
            "  'ت' :  19,\n",
            "  ...\n",
            "}\n"
          ]
        }
      ],
      "source": [
        "print('{')\n",
        "for char,_ in zip(char2idx, range(20)):\n",
        "    print('  {:4s}: {:3d},'.format(repr(char), char2idx[char]))\n",
        "print('  ...\\n}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l1VKcQHcymwb",
        "outputId": "92057e52-414b-48f3-a5b9-884339522db6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "'ای قبهٔ گردندهٔ بی روزن خضرا\\nب' ---- characters mapped to int ---- > [16 49  1 37 17 41 43  1 48 26 24 40 24 41 43  1 17 49  1 26 42 27 40  1\n",
            " 23 31 26 16  0 17]\n"
          ]
        }
      ],
      "source": [
        "# Show how the first 13 characters from the text are mapped to integers\n",
        "print ('{} ---- characters mapped to int ---- > {}'.format(repr(text[:30]), text_as_int[:30]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hgsVvVxnymwf"
      },
      "source": [
        "### Create training examples and targets\n",
        "\n",
        "Next divide the text into example sequences. Each input sequence will contain `seq_length` characters from the text.\n",
        "\n",
        "For each input sequence, the corresponding targets contain the same length of text, except shifted one character to the right.\n",
        "\n",
        "So break the text into chunks of `seq_length+1`. For example, say `seq_length` is 4 and our text is \"Hello\". The input sequence would be \"Hell\", and the target sequence \"ello\".\n",
        "\n",
        "To do this first use the `tf.data.Dataset.from_tensor_slices` function to convert the text vector into a stream of character indices."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "0UHJDA39zf-O"
      },
      "outputs": [],
      "source": [
        "# The maximum length sentence we want for a single input in characters\n",
        "seq_length = 100"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "H-zFw5O6-RDJ"
      },
      "outputs": [],
      "source": [
        "# Create training examples / targets\n",
        "char_dataset = tf.data.Dataset.from_tensor_slices(text_as_int)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RplQyLSA-RDJ",
        "outputId": "fef43856-3282-41f9-9cff-b1ce3b68d2c2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "16 ا\n",
            "49 ی\n",
            "1  \n",
            "37 ق\n",
            "17 ب\n",
            "41 ه\n",
            "43 ٔ\n",
            "1  \n",
            "48 گ\n",
            "26 ر\n"
          ]
        }
      ],
      "source": [
        "for i in char_dataset.take(10):\n",
        "    print(i.numpy(), idx2char[i.numpy()])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-ZSYAcQV8OGP"
      },
      "source": [
        "### Sliding Window for Sequence Generation\n",
        "\n",
        "Instead of using simple batching, we use the **sliding window** method to generate overlapping sequences of length `seq_length + 1`.  \n",
        "This ensures that every character in the text contributes to multiple training samples, preserving the continuity of the text.  \n",
        "Compared to fixed-size batching, this approach creates **richer and more context-aware training data**, helping the model better capture character-level dependencies in the text.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "l4hkDU3i7ozi"
      },
      "outputs": [],
      "source": [
        "# Sliding window: ساخت پنجره‌های طول ثابت با گام 1\n",
        "sequences = char_dataset.window(size=seq_length + 1, shift=1, drop_remainder=True)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# هر پنجره را به یک تانسور تبدیل می‌کنیم تا بتوان پردازشش کرد\n",
        "sequences = sequences.flat_map(lambda window: window.batch(seq_length + 1))"
      ],
      "metadata": {
        "id": "C1krUAxmGdpl"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UbLcIPBj_mWZ"
      },
      "source": [
        "For each sequence, duplicate and shift it to form the input and target text by using the `map` method to apply a simple function to each batch:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "9NGu-FkO_kYU"
      },
      "outputs": [],
      "source": [
        "def split_input_target(chunk):\n",
        "    input_text = chunk[:-1]\n",
        "    target_text = chunk[1:]\n",
        "    return input_text, target_text\n",
        "\n",
        "dataset = sequences.map(split_input_target)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hiCopyGZymwi"
      },
      "source": [
        "Print the first examples input and target values:"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# بررسی ۳ نمونه اولیه\n",
        "for input_example, target_example in dataset.take(3):\n",
        "    print('Input: ', repr(''.join(idx2char[input_example.numpy()])))\n",
        "    print('Target:', repr(''.join(idx2char[target_example.numpy()])))\n",
        "    print('-' * 30)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4ZbGZwW3GGEy",
        "outputId": "f945417e-55e4-49f6-b668-39af66535a78"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input:  'ای قبهٔ گردندهٔ بی روزن خضرا\\nبا قامت فرتوتی و با قوت برنا\\nفرزند توایم ای فلک، ای مادر بدمهر\\nای مادر '\n",
            "Target: 'ی قبهٔ گردندهٔ بی روزن خضرا\\nبا قامت فرتوتی و با قوت برنا\\nفرزند توایم ای فلک، ای مادر بدمهر\\nای مادر م'\n",
            "------------------------------\n",
            "Input:  'ی قبهٔ گردندهٔ بی روزن خضرا\\nبا قامت فرتوتی و با قوت برنا\\nفرزند توایم ای فلک، ای مادر بدمهر\\nای مادر م'\n",
            "Target: ' قبهٔ گردندهٔ بی روزن خضرا\\nبا قامت فرتوتی و با قوت برنا\\nفرزند توایم ای فلک، ای مادر بدمهر\\nای مادر ما'\n",
            "------------------------------\n",
            "Input:  ' قبهٔ گردندهٔ بی روزن خضرا\\nبا قامت فرتوتی و با قوت برنا\\nفرزند توایم ای فلک، ای مادر بدمهر\\nای مادر ما'\n",
            "Target: 'قبهٔ گردندهٔ بی روزن خضرا\\nبا قامت فرتوتی و با قوت برنا\\nفرزند توایم ای فلک، ای مادر بدمهر\\nای مادر ما '\n",
            "------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Create Training Batches\n",
        "\n",
        "After generating overlapping input-target pairs using the **sliding window**, we now prepare the data for efficient training.\n",
        "\n",
        "Using `tf.data.Dataset.batch`, we group these pairs into batches of size `BATCH_SIZE`.  \n",
        "This batching ensures optimized parallel processing on GPUs and uniform batch sizes by setting `drop_remainder=True`.  \n",
        "The final `dataset` contains batches of sequences, each ready to be fed into the model during training.\n"
      ],
      "metadata": {
        "id": "4sZO8XjxIfbe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "BATCH_SIZE = 32\n",
        "dataset = dataset.batch(BATCH_SIZE, drop_remainder=True)\n",
        "\n",
        "dataset"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rYVD48MIIgsa",
        "outputId": "6dea2269-1258-4565-b4ca-2575ffbfd5b3"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<_BatchDataset element_spec=(TensorSpec(shape=(32, None), dtype=tf.int64, name=None), TensorSpec(shape=(32, None), dtype=tf.int64, name=None))>"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Build the Model\n",
        "\n",
        "We define the model using `tf.keras.Sequential`, composed of three main layers:\n",
        "\n",
        "- **Embedding Layer**: Maps character indices into dense vectors of size `embedding_dim`. This layer is trainable and learns character-level embeddings.\n",
        "- **GRU Layer**: A type of recurrent neural network that captures sequence dependencies. We use `return_sequences=True` so the model outputs a prediction at each time step. The `stateful=True` option helps the model retain memory across batches for better long-term dependency learning.\n",
        "- **Dense Layer**: Outputs logits for each character in the vocabulary, one per time step.\n",
        "\n",
        "The model is built using the specified `vocab_size`, `embedding_dim`, and `rnn_units`.\n",
        "\n",
        "### Flexible Model Builder with GRU or LSTM\n",
        "\n",
        "We enhance the model-building function to support both GRU and LSTM layers, selectable via a `rnn_type` argument.\n",
        "\n",
        "- `GRU`: Efficient and faster to train\n",
        "- `LSTM`: Better for capturing long-term dependencies\n",
        "\n",
        "Dropout is added to reduce overfitting. Layer normalization is included after the RNN layer for better training stability.\n",
        "This setup allows easy experimentation and comparison between different RNN architectures.\n"
      ],
      "metadata": {
        "id": "iZ70ed46JFqT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Length of the vocabulary in chars\n",
        "vocab_size = len(vocab)\n",
        "\n",
        "# The embedding dimension\n",
        "embedding_dim = 256\n",
        "\n",
        "# Number of RNN units\n",
        "rnn_units = 1024"
      ],
      "metadata": {
        "id": "vxJJePqRJDWc"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def build_lstm_model(vocab_size, embedding_dim, rnn_units, batch_size, dropout_rate=0.2):\n",
        "    inputs = tf.keras.Input(batch_shape=(batch_size, None), dtype=tf.int32)\n",
        "\n",
        "    x = tf.keras.layers.Embedding(vocab_size, embedding_dim)(inputs)\n",
        "\n",
        "    x = tf.keras.layers.LSTM(\n",
        "        rnn_units,\n",
        "        return_sequences=True,\n",
        "        stateful=True,\n",
        "        dropout=dropout_rate,\n",
        "        recurrent_initializer='glorot_uniform'\n",
        "    )(x)\n",
        "\n",
        "    x = tf.keras.layers.LayerNormalization()(x)\n",
        "    outputs = tf.keras.layers.Dense(vocab_size)(x)\n",
        "\n",
        "    model = tf.keras.Model(inputs, outputs)\n",
        "    return model\n"
      ],
      "metadata": {
        "id": "pBDUdtwiJH36"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_lstm = build_model(\n",
        "    vocab_size=vocab_size,\n",
        "    embedding_dim=embedding_dim,\n",
        "    rnn_units=rnn_units,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    rnn_type='LSTM'\n",
        ")"
      ],
      "metadata": {
        "id": "IG1p9TwhJIjE"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Try the Model\n",
        "\n",
        "Before training, we can run the model on a sample input batch to verify its behavior.\n",
        "\n",
        "- The model outputs a tensor of shape `(batch_size, sequence_length, vocab_size)`.\n",
        "- To simulate actual text generation, we sample from the output distribution at each time step using `tf.random.categorical`, instead of taking `argmax`, to avoid repetitive predictions.\n",
        "- Finally, we decode the predicted character indices to see the raw output of the untrained model.\n"
      ],
      "metadata": {
        "id": "yjHQ7YC0K-sU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for input_example_batch, target_example_batch in dataset.take(1):\n",
        "    example_batch_predictions = model_gru.predict(input_example_batch)\n",
        "    print(example_batch_predictions.shape, \"# (batch_size, sequence_length, vocab_size)\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IS5ZY5vCKej2",
        "outputId": "5252ff1c-f55c-4a01-cbb1-7e8a7188d5b6"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 167ms/step\n",
            "(32, 100, 50) # (batch_size, sequence_length, vocab_size)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "example_batch_predictions"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-l5B557LO0B-",
        "outputId": "e05a4c9c-022c-43bd-903e-ad0ff43aa96c"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[[ -0.5249215 ,   2.3918264 ,  -7.1202517 , ...,  -1.3167013 ,\n",
              "          -1.5620731 ,   1.0766512 ],\n",
              "        [  0.92533475,   4.5751505 ,  -2.1935277 , ...,  -2.408623  ,\n",
              "          -1.4269058 ,  -1.8004861 ],\n",
              "        [ -3.5202813 ,  -2.406284  ,  -3.0853682 , ...,   2.2221081 ,\n",
              "           1.5361933 ,  -0.11064744],\n",
              "        ...,\n",
              "        [ -0.3903244 ,   1.8764384 ,  -7.3837485 , ...,  -1.6681508 ,\n",
              "           1.0220612 ,  -0.39580935],\n",
              "        [  0.8346012 ,   4.417794  ,  -7.175815  , ...,  -3.0666664 ,\n",
              "          -1.3717612 ,   0.90533733],\n",
              "        [ -3.5669644 ,  -2.6329465 ,  -5.0795445 , ...,   2.2521503 ,\n",
              "           1.518972  ,   0.1873501 ]],\n",
              "\n",
              "       [[  0.87647027,   3.5626388 ,  -5.010244  , ...,   3.0610664 ,\n",
              "          -1.7341214 ,  -2.6249466 ],\n",
              "        [ -4.1322823 ,  -2.9421353 ,  -4.549287  , ...,   2.796818  ,\n",
              "           1.4297993 ,  -0.24430805],\n",
              "        [ -5.850661  ,  -2.8289795 ,  -7.173169  , ...,  -1.6420285 ,\n",
              "           0.01687189,   2.2343833 ],\n",
              "        ...,\n",
              "        [  1.3017039 ,   4.6325607 ,  -7.0779815 , ...,  -4.259546  ,\n",
              "          -2.0068517 ,   1.3022624 ],\n",
              "        [ -3.2479992 ,  -2.5540342 ,  -4.7810297 , ...,   2.1142025 ,\n",
              "           1.4706005 ,   0.06239951],\n",
              "        [ -3.346451  ,  -1.9623365 ,  -6.3248606 , ...,   1.526639  ,\n",
              "           0.45981377,   1.7001789 ]],\n",
              "\n",
              "       [[ -3.8471396 ,  -2.7432368 ,  -5.9258785 , ...,   2.2283633 ,\n",
              "           1.11247   ,  -0.25207675],\n",
              "        [ -6.583101  ,  -3.2100983 ,  -8.1104355 , ...,  -1.5877426 ,\n",
              "          -0.04933624,   2.3124294 ],\n",
              "        [ -3.7678552 ,   0.1827284 , -10.847387  , ...,  -3.221261  ,\n",
              "          -2.1987374 ,   1.6392663 ],\n",
              "        ...,\n",
              "        [ -3.5045645 ,  -2.613515  ,  -4.9805765 , ...,   2.426841  ,\n",
              "           1.4455726 ,   0.14004087],\n",
              "        [ -3.6231236 ,  -1.8831367 ,  -6.4439993 , ...,   1.6193016 ,\n",
              "           0.39287472,   1.6378725 ],\n",
              "        [ -0.88485205,   1.9149514 ,  -5.45354   , ...,   0.30006444,\n",
              "          -1.1954925 ,  -1.4031081 ]],\n",
              "\n",
              "       ...,\n",
              "\n",
              "       [[ -1.2097986 ,   0.84715056,  -8.694102  , ...,  -0.8598676 ,\n",
              "          -2.296319  ,   0.11979303],\n",
              "        [ -1.9743345 ,   2.6462414 ,  -5.8137774 , ...,  -1.225479  ,\n",
              "          -2.4110298 ,   0.904099  ],\n",
              "        [ -3.5529754 ,  -2.2149532 ,  -3.8887506 , ...,   2.472836  ,\n",
              "           1.0767831 ,   0.05108261],\n",
              "        ...,\n",
              "        [  8.3652525 ,   2.836067  ,  -0.84901536, ...,  -1.7570336 ,\n",
              "          -3.4890153 ,   0.9183836 ],\n",
              "        [ -2.8687372 ,  -2.135459  ,  -2.5563362 , ...,   1.5375075 ,\n",
              "           1.8525224 ,  -0.20316333],\n",
              "        [ -5.979049  ,  -3.478418  , -10.207657  , ...,   0.9516732 ,\n",
              "           1.0818502 ,  -0.39793336]],\n",
              "\n",
              "       [[ -3.8855703 ,  -0.81321406,  -7.323014  , ...,   0.4946646 ,\n",
              "           1.3074048 ,   2.627614  ],\n",
              "        [ -3.541994  ,  -2.5826385 ,  -3.0964746 , ...,   3.0418043 ,\n",
              "           1.3992822 ,  -0.63291925],\n",
              "        [ -6.115683  ,  -2.9218254 ,  -7.503677  , ...,  -1.6890371 ,\n",
              "           0.14141858,   2.195996  ],\n",
              "        ...,\n",
              "        [ -2.7661345 ,  -2.127872  ,  -2.4847322 , ...,   1.6422529 ,\n",
              "           1.7778089 ,  -0.30814713],\n",
              "        [ -5.8033104 ,  -3.4867356 ,  -9.999122  , ...,   1.0510837 ,\n",
              "           0.95780873,  -0.28904694],\n",
              "        [ -1.4258454 ,   1.428317  , -11.238631  , ...,  -1.3103157 ,\n",
              "          -1.9365443 ,   0.25568882]],\n",
              "\n",
              "       [[ -3.6802511 ,  -2.3545094 ,  -5.566709  , ...,   1.7404975 ,\n",
              "           1.358835  ,  -0.43489325],\n",
              "        [ -6.1123934 ,  -3.2679384 ,  -8.038743  , ...,  -1.4937915 ,\n",
              "          -0.12126164,   2.6473515 ],\n",
              "        [ -2.9960918 ,   0.6358818 ,  -7.582828  , ...,  -1.7894613 ,\n",
              "          -2.083135  ,   1.3886676 ],\n",
              "        ...,\n",
              "        [ -5.7155085 ,  -3.4431276 ,  -9.948723  , ...,   1.0696526 ,\n",
              "           0.81945825,  -0.32382017],\n",
              "        [ -1.5654266 ,   1.1956576 , -11.066858  , ...,  -1.5725161 ,\n",
              "          -1.9378034 ,   0.45615062],\n",
              "        [ -1.0509864 ,   1.6601386 ,  -6.9293933 , ...,  -3.6414163 ,\n",
              "          -0.5875735 ,   0.8188181 ]]], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_lstm.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 312
        },
        "id": "miPS7z2aNUdl",
        "outputId": "1897839d-1105-4ce7-9fc2-9a71e6fb997f"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"functional_2\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_2\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ input_layer_2 (\u001b[38;5;33mInputLayer\u001b[0m)      │ (\u001b[38;5;34m32\u001b[0m, \u001b[38;5;45mNone\u001b[0m)             │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ embedding_2 (\u001b[38;5;33mEmbedding\u001b[0m)         │ (\u001b[38;5;34m32\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)        │        \u001b[38;5;34m12,800\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ lstm_1 (\u001b[38;5;33mLSTM\u001b[0m)                   │ (\u001b[38;5;34m32\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1024\u001b[0m)       │     \u001b[38;5;34m5,246,976\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ layer_normalization_2           │ (\u001b[38;5;34m32\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1024\u001b[0m)       │         \u001b[38;5;34m2,048\u001b[0m │\n",
              "│ (\u001b[38;5;33mLayerNormalization\u001b[0m)            │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_2 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;34m32\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m50\u001b[0m)         │        \u001b[38;5;34m51,250\u001b[0m │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ input_layer_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)      │ (<span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)             │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ embedding_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)         │ (<span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)        │        <span style=\"color: #00af00; text-decoration-color: #00af00\">12,800</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ lstm_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                   │ (<span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1024</span>)       │     <span style=\"color: #00af00; text-decoration-color: #00af00\">5,246,976</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ layer_normalization_2           │ (<span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1024</span>)       │         <span style=\"color: #00af00; text-decoration-color: #00af00\">2,048</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LayerNormalization</span>)            │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">50</span>)         │        <span style=\"color: #00af00; text-decoration-color: #00af00\">51,250</span> │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m5,313,074\u001b[0m (20.27 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">5,313,074</span> (20.27 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m5,313,074\u001b[0m (20.27 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">5,313,074</span> (20.27 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sampled_indices = tf.random.categorical(\n",
        "    logits=example_batch_predictions[0],  # shape = (sequence_length, vocab_size)\n",
        "    num_samples=1\n",
        ")"
      ],
      "metadata": {
        "id": "7CecfU88OCyi"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sampled_indices = tf.squeeze(sampled_indices, axis=-1).numpy()"
      ],
      "metadata": {
        "id": "yhzZejBNOjwU"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sampled_indices"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nhvE9BVHO_Js",
        "outputId": "ebe6ece8-7275-4c59-c6ba-fe9fb15ad01a"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([ 1, 39, 39, 16, 49, 43,  1, 26, 36, 36,  0,  1,  0,  0,  0, 19, 34,\n",
              "        0, 38, 42, 40,  1,  1, 16, 41, 38,  1, 40, 16, 24, 32, 24, 30, 47,\n",
              "       41,  1, 39, 34,  1, 42, 40,  1,  1, 17,  1, 24, 49, 29, 28, 36, 38,\n",
              "       41, 30, 48,  1, 16,  0, 39, 33, 28, 40, 24,  0, 16, 41,  0, 40, 24,\n",
              "       16, 16, 28, 40, 38, 26, 16,  0,  1, 40, 27, 40, 39, 40, 40, 26,  1,\n",
              "       48, 30,  1,  0, 43,  9, 49, 48,  1, 45, 26, 24, 26,  1, 23])"
            ]
          },
          "metadata": {},
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "input_text = ''.join([idx2char[i] for i in input_example_batch[0].numpy()])"
      ],
      "metadata": {
        "id": "mhfLPjXsOmLg"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predicted_text = ''.join([idx2char[i] for i in sampled_indices])"
      ],
      "metadata": {
        "id": "WfIAF_EVOmd8"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Input: \\n\", repr(input_text))\n",
        "print()\n",
        "print(\"Next Char Predictions: \\n\", repr(predicted_text))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PAxNBc8SOqvA",
        "outputId": "400b1974-f87f-4216-9f4c-5d261a94948c"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input: \n",
            " 'ای قبهٔ گردندهٔ بی روزن خضرا\\nبا قامت فرتوتی و با قوت برنا\\nفرزند توایم ای فلک، ای مادر بدمهر\\nای مادر '\n",
            "\n",
            "Next Char Predictions: \n",
            " ' ممایٔ رفف\\n \\n\\n\\nتع\\nلون  اهل نادطدصکه مع ون  ب دیشسفلهصگ ا\\nمظسند\\nاه\\nندااسنلرا\\n نزنمننر گص \\nٔ،یگ چردر خ'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Train the model"
      ],
      "metadata": {
        "id": "olquFPcFRAUn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Loss Function and Model Compilation\n",
        "\n",
        "We define a custom loss function using `sparse_categorical_crossentropy`, suitable for multi-class classification tasks with integer labels.  \n",
        "Since the model outputs raw logits (not softmax probabilities), we set `from_logits=True`.\n",
        "\n",
        "We then compile the model using the Adam optimizer and our loss function to prepare it for training.\n"
      ],
      "metadata": {
        "id": "Qb9zEFtDMlfB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def loss(labels, logits):\n",
        "    return tf.keras.losses.sparse_categorical_crossentropy(labels, logits, from_logits=True)"
      ],
      "metadata": {
        "id": "P2_8fi6BQ_Lr"
      },
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "example_batch_loss = loss(target_example_batch, example_batch_predictions)"
      ],
      "metadata": {
        "id": "lGd1jplJM2TY"
      },
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Prediction shape: \", example_batch_predictions.shape, \" # (batch_size, sequence_length, vocab_size)\")\n",
        "print(\"Example batch loss (mean):\", tf.reduce_mean(example_batch_loss).numpy())"
      ],
      "metadata": {
        "id": "geW6mEf3Q-SP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "428d7011-956f-434c-aecf-ed788cb2fbe3"
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prediction shape:  (32, 100, 50)  # (batch_size, sequence_length, vocab_size)\n",
            "Example batch loss (mean): 2.1003053\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# LSTM\n",
        "model_lstm.compile(\n",
        "    optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n",
        "    loss=loss,\n",
        "    metrics=[tf.keras.metrics.SparseCategoricalAccuracy()]\n",
        ")"
      ],
      "metadata": {
        "id": "1w7tZdVmMzmE"
      },
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Configuring Checkpoints\n",
        "\n",
        "Each model (GRU and LSTM) is trained separately and stores its weights in a dedicated checkpoint directory.  \n",
        "We use `ModelCheckpoint` with `save_weights_only=True` to save only the model parameters after each epoch.  \n",
        "This setup allows independent training, evaluation, and recovery for each architecture.\n"
      ],
      "metadata": {
        "id": "jsWKFuYMPK62"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# مسیر ذخیره برای مدل LSTM\n",
        "checkpoint_dir_lstm = r\"/content/drive/MyDrive/Colab Notebooks/NLP/2/10-Text Generator(Attar's Poem)/Sample/checkpoint/training_checkpoints_lstm\"\n",
        "checkpoint_prefix_lstm = os.path.join(checkpoint_dir_lstm, \"ckpt_{epoch}.weights.h5\")\n",
        "\n",
        "checkpoint_callback_lstm = tf.keras.callbacks.ModelCheckpoint(\n",
        "    filepath=checkpoint_prefix_lstm,\n",
        "    save_weights_only=True\n",
        ")"
      ],
      "metadata": {
        "id": "EF4eG-OiOO-S"
      },
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Smart Training Wrapper\n",
        "\n",
        "We define a `train_model` function that automatically checks whether the dataset is repeated.  \n",
        "If repeated, it computes and sets `steps_per_epoch` accordingly.  \n",
        "This makes the training process more robust and avoids infinite training loops.\n"
      ],
      "metadata": {
        "id": "4XeGzfsOSC4o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_model(model, dataset, checkpoint_callback,\n",
        "                text_as_int_len, seq_length, batch_size,\n",
        "                epochs=10):\n",
        "\n",
        "    steps_per_epoch = (text_as_int_len - (seq_length + 1)) // batch_size\n",
        "    print(f\"✅ Computed steps_per_epoch = {steps_per_epoch}\")\n",
        "\n",
        "    history = model.fit(\n",
        "        dataset,\n",
        "        epochs=epochs,\n",
        "        steps_per_epoch=steps_per_epoch,\n",
        "        callbacks=[checkpoint_callback]\n",
        "    )\n",
        "\n",
        "    return history"
      ],
      "metadata": {
        "id": "2-OejZAESCUD"
      },
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "EPOCHS = 10\n",
        "\n",
        "history_lstm = train_model(\n",
        "    model=model_lstm,\n",
        "    dataset=dataset,\n",
        "    checkpoint_callback=checkpoint_callback_lstm,\n",
        "    text_as_int_len=len(text_as_int),\n",
        "    seq_length=seq_length,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    epochs=EPOCHS\n",
        "\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VJRG8XApNXfW",
        "outputId": "8c78f911-ab47-4133-b4e9-25b5495294a1"
      },
      "execution_count": 63,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ Computed steps_per_epoch = 19791\n",
            "Epoch 1/10\n",
            "\u001b[1m19791/19791\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m996s\u001b[0m 50ms/step - loss: 2.0306 - sparse_categorical_accuracy: 0.4199\n",
            "Epoch 2/10\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/trainers/epoch_iterator.py:151: UserWarning: Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches. You may need to use the `.repeat()` function when building your dataset.\n",
            "  self._interrupted_warning()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m19791/19791\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12us/step - loss: 0.0000e+00 - sparse_categorical_accuracy: 0.0000e+00\n",
            "Epoch 3/10\n",
            "\u001b[1m19791/19791\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1002s\u001b[0m 51ms/step - loss: 1.7884 - sparse_categorical_accuracy: 0.4799\n",
            "Epoch 4/10\n",
            "\u001b[1m19791/19791\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11us/step - loss: 0.0000e+00 - sparse_categorical_accuracy: 0.0000e+00\n",
            "Epoch 5/10\n",
            "\u001b[1m19791/19791\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1040s\u001b[0m 51ms/step - loss: 1.7424 - sparse_categorical_accuracy: 0.4919\n",
            "Epoch 6/10\n",
            "\u001b[1m19791/19791\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12us/step - loss: 0.0000e+00 - sparse_categorical_accuracy: 0.0000e+00\n",
            "Epoch 7/10\n",
            "\u001b[1m19791/19791\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m998s\u001b[0m 50ms/step - loss: 1.7132 - sparse_categorical_accuracy: 0.4984\n",
            "Epoch 8/10\n",
            "\u001b[1m19791/19791\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13us/step - loss: 0.0000e+00 - sparse_categorical_accuracy: 0.0000e+00\n",
            "Epoch 9/10\n",
            "\u001b[1m19791/19791\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1051s\u001b[0m 51ms/step - loss: 1.6849 - sparse_categorical_accuracy: 0.5061\n",
            "Epoch 10/10\n",
            "\u001b[1m19791/19791\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11us/step - loss: 0.0000e+00 - sparse_categorical_accuracy: 0.0000e+00\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_text_from_checkpoint(checkpoint_path, start_string, build_fn, char2idx, idx2char,\n",
        "                                  temperature=1.0, num_generate=1000):\n",
        "    \"\"\"\n",
        "    Generate text using a trained model checkpoint.\n",
        "\n",
        "    Parameters:\n",
        "    - checkpoint_path: str\n",
        "        Path to the saved `.weights.h5` file.\n",
        "    - start_string: str\n",
        "        Initial seed string to start text generation.\n",
        "    - build_fn: function\n",
        "        A function to build the model (e.g., build_lstm_model) with batch_size=1.\n",
        "    - char2idx: dict\n",
        "        Dictionary mapping characters to integer indices.\n",
        "    - idx2char: np.array or list\n",
        "        List or array mapping indices back to characters.\n",
        "    - temperature: float\n",
        "        Sampling randomness. Lower = more conservative, higher = more diverse.\n",
        "    - num_generate: int\n",
        "        Total number of characters to generate after the seed string.\n",
        "\n",
        "    Returns:\n",
        "    - str\n",
        "        Full generated text string starting from `start_string`.\n",
        "    \"\"\"\n",
        "    model = build_fn(\n",
        "        vocab_size=len(idx2char),\n",
        "        embedding_dim=256,\n",
        "        rnn_units=1024,\n",
        "        batch_size=1\n",
        "    )\n",
        "    model.load_weights(checkpoint_path)\n",
        "    model.build(tf.TensorShape([1, None]))\n",
        "\n",
        "    # Find the LSTM layer to reset its state\n",
        "    for layer in model.layers:\n",
        "        if isinstance(layer, tf.keras.layers.LSTM):\n",
        "            layer.reset_states()\n",
        "            break  # Only reset the first LSTM (common case)\n",
        "\n",
        "    # Convert seed string to input tensor\n",
        "    input_eval = [char2idx[s] for s in start_string]\n",
        "    input_eval = tf.expand_dims(input_eval, 0)\n",
        "\n",
        "    text_generated = []\n",
        "\n",
        "    for _ in range(num_generate):\n",
        "        predictions = model(input_eval)\n",
        "        predictions = tf.squeeze(predictions, 0) / temperature\n",
        "        predicted_id = tf.random.categorical(predictions, num_samples=1)[-1, 0].numpy()\n",
        "        input_eval = tf.expand_dims([predicted_id], 0)\n",
        "        text_generated.append(idx2char[predicted_id])\n",
        "\n",
        "    return start_string + ''.join(text_generated)\n"
      ],
      "metadata": {
        "id": "L6TsYOyY1Bjm"
      },
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "# لیست همه فایل‌ها در پوشه‌ی checkpoint\n",
        "checkpoint_files = sorted([\n",
        "    f for f in os.listdir(checkpoint_dir_lstm)\n",
        "    if f.endswith(\".weights.h5\")\n",
        "])\n",
        "\n",
        "# گرفتن جدیدترین فایل\n",
        "latest_checkpoint = os.path.join(checkpoint_dir_lstm, checkpoint_files[-1])\n",
        "print(f\"🧠 Latest checkpoint: {latest_checkpoint}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mmEE10pD3TcK",
        "outputId": "49677feb-3eec-4a14-f5d3-220b3c86a96e"
      },
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🧠 Latest checkpoint: /content/drive/MyDrive/Colab Notebooks/NLP/2/10-Text Generator(Attar's Poem)/Sample/checkpoint/training_checkpoints_lstm/ckpt_9.weights.h5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "generated_text = generate_text_from_checkpoint(\n",
        "    checkpoint_path=latest_checkpoint,\n",
        "    start_string=\"ای دل: \",\n",
        "    build_fn=build_lstm_model,\n",
        "    char2idx=char2idx,\n",
        "    idx2char=idx2char,\n",
        "    temperature=0.8,\n",
        "    num_generate=1000\n",
        ")\n",
        "\n",
        "print(generated_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QWELso6k2vlQ",
        "outputId": "a6500683-d806-4e52-f42b-e5a0a3649646"
      },
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ای دل: دوروی\n",
            "اگر بند اامسی که مقرون کجائی\n",
            "چه طمع خورشید دهم خواعد جان است\n",
            "تو رف جهل همه جمله ای امروز\n",
            "از بهر صومرا در\n",
            "نتوانی امام ایمن\n",
            "تو امروز بی طمع تا رفت در این ام\n",
            "مرا یار فرمانده سخن تاج اند\n",
            "تا گوئی که قران روان همی ایمن\n",
            "ز بهر مردم نفس در آر او را\n",
            "هر احسان چو تو امامه ای مرا مادر\n",
            "ای\n",
            "از گهر آمد\n",
            "از من فزون از اول همچون خوی؟\n",
            "دشمن در این اندر سوی آن درون دون کاج تو\n",
            "مرد عاجز در این افگند جوان\n",
            "سبک در جهان داد بر این جهان مرد\n",
            "ای عامه مادر دیم از بلب او اند\n",
            "شاه گمتا بر جان تو بد افتخار و بهمان\n",
            "اگر در این مکه اندر این سخن باغبان درو بی فنی ثی\n",
            "از محل الواسوی امروز\n",
            "فرزند این جهان مهمان چون تو ک\n",
            "به آهخته ای چرخ چنان تا پشتی\n",
            "معدهٔ مقرون ای بدمخواه و جان است\n",
            "این سپذر\n",
            "تو دریا کنی عهد جسمی و تن\n",
            "اسپ ما بر خرد در دعام\n",
            "در تو ماور در بمان شد شد کان\n",
            "همچون ز کلام او دواقوال اندر فائی\n",
            "ای درد\n",
            "معصفرمان از دل تو فربه در جمالی\n",
            "ای حجت\n",
            "اند\n",
            "هرگز از زاعت ماه تا همه\n",
            "ای مادر ا تو بی نظیر\n",
            "به دین مده او سرت سؤال ظوی\n",
            "ورنه در دل اهل صف تا مر در زمین م\n",
            "ایزد\n",
            "بسیار یابگان تو جهان با مؤذن افگند\n",
            "درویش و آب سؤالت اعدام\n",
            "ای گمان آم\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}